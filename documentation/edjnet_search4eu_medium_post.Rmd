---
title: "Building a journalist's ideal librarian"
subtitle: "exploring and improving the Aleph stack for journalistic open source research"
author: "Sylvain Lapoix et Mathieu Morey (Datactivist)"
date: "29/04/2021"
output: word_document
---

This piece details the work of the Datactivist team with the European Data Journalism Network (EDJNet) to master and use Aleph to build a search engine for public documents, regardless of their format or language.

The repo containing the code for the search4eu project is available under CC licence.
We also assembled a user guide as a wiki for the repo for those who would like to use Aleph for their own crawling and indexation projects.

We hereby thank the EDJNet team for their trust, support and precious feedback : it was the perfect place to explore this tool.

We thank and praise the wonderful team at OCCRP who put up this game changing tool and welcomed, listened and helped us all along : your work is gold.

# Introduction

Sylvain discovered the Aleph stack at the 2018 Open Government Partnership summit in Tbilisi (Georgia). After a pretty classic presentation on open source investigation, he sat at the table of a digital nomad / dev working at the time for OCCRP in Sarajevo who fliped her laptop open at the black interface he'd become familiar with : a search engine to make requests across several databases covering pretty much every country in the world.

In the meantime, Mathieu was working on the best ways to explore administrative acts. Because of legal obligations (for example, the need to use the signed version of a city council's decision to make it valid), most of those were published in image-based PDF format, preventing indexing and search. Hence, this trove was technically locked for any search engine to map the terms the docs contained.

Both cooperators for Datactivist, an open data consulting company based in Provence, they were discussing their researches at the Ecomotive café, down the great white stairs of the Saint-Charles station in Marseille. It's only after a couple of hours that they realised that one could have the key to the safe the other was trying to open. Soon after, Gian-Paolo Accardo came to Sylvain asking if he would join the European Datajournalism Network to help build new tools for investigation. Mathieu and Sylvain offered to adapt and apply the Aleph stack to European topics and sources. The EDJNet board was in. And so the lock-picking season began.

# Using Aleph to index batch of files

Our first attempt consisted in using Aleph as a local search engine : feeding it with batches of PDF files and letting it sort them out, reading their content and connecting the dots to help our research through them ... a sort of home made librarian.

This feature is actually the first intent of Aleph. The original OCCRP's Aleph looks up more than 300 publics datasets and leaks, from the German companies registry to the blog posts of the assassinated Maltese reporter Daphne Caruana-Galizia's blog or Wikileaks publications. It is, by itself, an open air gold mine.

Mathieu and Sylvain already had a batch of files to dig in. They decided to come in by the easy path : throw them at Aleph and see how it chew it.

## Our approach

We decided to go slow by using Aleph locally first. It meant installing Aleph, updating it and launching it from our command line to get a local URL that could only be accessed from our computer.

[...]

## Lessons learned

[...]

To learn how to do it yourself, check the Aleph section of our user guide.

# Using Memorious to extract public documents

We decided to turn to Memorious, the "crawling" tool of the Aleph stack, to retrieve automatically big batches of documents from public websites. In practical terms, Memorious allows you to manage small programs (the crawlers) that will follow a given path to retrieve documents. Those crawlers can be designed with a luxury of functions and parameters following several steps : generating urls, connecting to services, following links, cleaning content, storing documents, etc. Of course, you can also create function of your own in Python (and that's what Mathieu did, on several occasions, to fill the gaps).

We wanted to harness the power of Memorious to help our colleagues from Civio investigate the sitution of access to mental healthcare. And we aimed too high for a start : retrieve all the public decisions regarding health in the national and provincial legislature available online.

In practice, it meant designing one crawler per website that would generate the url for a decision, retrieve the page, extract from the layout the part of the page on the decision, clean it and store it. Easier said than done.

## Our approach

We first asked our colleague from Civio where to look for. She provided us with a long list of websites, mainly national and local authorities : Boletin Oficial del Estado (the Spanish Official journal), the equivalent for each autonomous provinces and cities, the Spanish Congress website, etc.

We decided to focus first on the official gazettes, as they represented the most promising source of documents with the more variety.

The first difficulty we encountered was that every website had its own structure that reflected into the shape of the URL of the decisions. Place of publication for the deliberation of the Junta of Andalucia, the Boletín Oficial de la Junta de Andalucía (which we ended up calling the Boja) labeled each URL with the year and day of publicaton. But, some days has no decision published. Fortunately, Memorious had this covered : with `init` methods, we could generate the pieces of the URL, adding one day on year down or up, and paste them together to look up for legitimate URL. We had to do it for each website. And, here, we deeply regretted our Spanish classes.

But this part was the easy one. Once we got to the page itself, we needed to target the right link to follow to get to the docs. [...]

## Lessons learned

Back from this Spanish expedition, we came to understand the importance of mastering every step of the pipeline and the versatility of it : the many options offered by the already existing functions and the level of detail you can specify what you keep and what you stash came as essential tools to use the crawler in website as complex as those.

Second important thing : we would have greatly benefited from a good guide in this mess ! At the crossroads of legal terminology and local parameters, those pages may look as an amusement park for an experienced hiker of the Spanish legislative path. For us, it was a foggy maze of exotic references and strange labels. **Knowing your way in a website is not something to learn while programming a crawler : it's a prerequisite.**

We compiled the step-by-step process of Memorious installation, crawler setup and running in the Memorious section of our user guide.

# Combining Memorious and Aleph to create a better search engine for public websites

We now had experienced the ingestion process as well as the crawling. Combining the two would allow us to design a custom document search engine, updating itself and indexing documents on the fly. Our original intent, in a way. But, for this, we needed two things :

1. a source of documents narrow enough to be manageable but diverse enough to cover several topics and countries at once to be useful to as many journalists as possible ;
2. a source that didn't already feature a search engine or that featured a search engine that didn't fully meet the needs of journalistic methodology (we didn't want to make a duplicate internal search engine for a public database).

And so, we decided to use the Aleph stack to improve the search on one of the most European topics of all : competition.

The competition website of the European Commission is a story-rich maze : behind each decision or step of the procedure lies potential cartel, free market rules infrigement and interesting power plays between companies. Covering all the economic sectors and the life span of the EU, it suffers from a search engine that doesn't meet the needs of journalistic research :

* the company names are not indexed as a whole but token by token (see the example below) ;
* worse : the relevant entities mentionned inside the docs (either web page or docx and pdf files) are not indexed. Meaning you can only search for tokens in the title of the decisions or documents.

The topic and potential was worth the work. So we got to it.

## Our approach

As usual, the first step was to learn how to navigate the site. Among its particularities, it is made of a different fabric depending on the type of decision you look at : each type (antitrust/cartels, mergers and state aid) displays its content in a different manner. That called for a set of three different crawlers, one for each type. Yeah : even on the same website, you sometime need to custom made the crawler for specific targets. We decided to start with the best organized and most promissing from a journalistic perspective : antitrust and cartels.

The other problem we had was that the URLs didn't follow a clear pattern : counting down from a very high case number, they skipped hundreds and hundreads of steps. Memorious can handle missing pages, it does it very well, but each page had to be called, downloaded, analyzed ... and that consumed time, energy and storage. So we decided to use another method : bouncing from pages to pages.

As Memorious works a lot by following links, we decided to target those that pointed at lists of cases : the economical sector of the companies involved, the name of the companies, the related cases ... the pages returned by those links were full of other cases, sometimes not sorted in chronological order, allowing us to jump from a 2020 cartel investigation to a 1980 business deal. This "bouncing bot" was able to gather as much as [XXXXXX] documents per hour.

All those docs were fed directly to Aleph via the Aleph connect module we learned to use (and love). But it was still missing something : the document came with no metadata. No date, no company name, sector or even category of inquiry (cartel or antitrust). Aleph couldn't use anything to index it the proper way.

So Mathieu digged in the toolbox. So far, we focused on how to use the crawler and their function as they came. There was a lot of useful stuff, improved by other users. But here, we needed an extra layer of code to do what we needed : make Memorious snatch some data about the document in the page that contained the link. Those tables I mentioned earlier had it all : company name, type of case, date of publication ... so, Mathieu made a new series of functions of his own, the "competition" set, that tooks those data and passed them to be stored with the doc.

We now had the docs but also the right company names, dates and sectors ... those things we missed so much from the original search engine.

## Lessons learned

The first thing we realised here is that Aleph can be very user friendly. During our previous attempts, we sweat at building our crawlers and then took hours to analyze the docs they brought us back ... Now we knew out to connect them with each other, we could see the result poping up in real time in the interface !

The second important piece of advice : it's OK if it's messy, it will get sorted later. Crawling docs is hard but, if you get all the metadata you need, it will tidy up soon. Hence, following all the leads you have on a page can be a much more efficient approach to a website that trying desperately to reproduce its architecture.

And last but not least : Aleph and Memorious are made for you to improve. We were stuck for weeks in the set of function and pipeline we read about in the documentation, following them with care, binding our brains to fit squares into circles. In the end, building our own tools and inserting them in this framework was easier and liberating. This stack is solid enough to play with as you intent to.

You'll find technical details on how to connect Memorious to Aleph and index your docs on the fly in a dedicated sub section of the user guide.

# Conclusion and follow up

The Aleph stack made us dream big. From local government decisions to national health regulation or Continent-wide inquiry, we embarqued in a journey into open docs repository that made us realise the potentiel of this tool for journalists (and it's huge !). But the most important lesson we learned is : every step to set up those tools requires a lot of expertise, exploration and time.

Aleph accomplishes several tasks no single journo (or even newsroom) could achieve by itself :

* searching through complex paths for docs of all sorts ;
* extracting data on the fly, regardless of the format ;
* using the metadata and content to sort those documents and connect them, inside and across databases.

The skills required to leverage each of those capability are diverse :

* back end dev to setup the tools and manage the databases ;
* front end dev to improve the access and build new entry points for the database ;
* natural langue experts to 
* and journalists, of course, to assert the relevance, focus on what matters and make use of it.

The OCCRP team worked hard to make Aleph exist. This organisation needs more of those specialist to help it achieve what it can offer to journalists.

## Next steps

